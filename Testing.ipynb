{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOtCnbkqUr6cHxBSxMevhfa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a12545d86554b8dbdc2d40429543446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee21f3a734174b7d8d28ad80ea6396ef",
              "IPY_MODEL_8530792dc4f5451fb7b8571eb8b82710",
              "IPY_MODEL_7757d2ad52b344418dc0d1680da84a4d"
            ],
            "layout": "IPY_MODEL_d9b3be75ac7b4223ae3c7ddcde5c325d"
          }
        },
        "ee21f3a734174b7d8d28ad80ea6396ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2310c4196e5b4a7ea495f20b01c36a4d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e3a4f0c4039044ebbd1074983d6c1147",
            "value": "open_clip_model.safetensors:‚Äá100%"
          }
        },
        "8530792dc4f5451fb7b8571eb8b82710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_109138de098e44b2b0131d08fd758de0",
            "max": 605143284,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e597fc19aa44a92af1056901c7bb281",
            "value": 605143284
          }
        },
        "7757d2ad52b344418dc0d1680da84a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2009258dd6ee4dcb9a17496fa286aa3c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e28ad1a9fffa4f3b8924dbe6efc3dc7b",
            "value": "‚Äá605M/605M‚Äá[00:07&lt;00:00,‚Äá114MB/s]"
          }
        },
        "d9b3be75ac7b4223ae3c7ddcde5c325d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2310c4196e5b4a7ea495f20b01c36a4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3a4f0c4039044ebbd1074983d6c1147": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "109138de098e44b2b0131d08fd758de0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e597fc19aa44a92af1056901c7bb281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2009258dd6ee4dcb9a17496fa286aa3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e28ad1a9fffa4f3b8924dbe6efc3dc7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkhilaNacham/MedICat/blob/main/Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VNQx72a0ExP",
        "outputId": "a0b52680-692e-4d35-a861-6ecd67bb1c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.119.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.3)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.19.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.37.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch torchvision pillow tqdm gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKbv9mFL1JRp",
        "outputId": "ac07d0eb-509f-4a27-b439-103d30176780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import gradio as gr\n",
        "\n",
        "# --- DATA LAYER ---\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/MediCat/figures\"\n",
        "ANNOT_PATH = \"/content/drive/MyDrive/MediCat/subcaptions_public.jsonl\""
      ],
      "metadata": {
        "id": "6M4PEMxN1SkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_request(query):\n",
        "    \"\"\"User Layer: Request Handler\"\"\"\n",
        "    if not query.strip():\n",
        "        return None, \"‚ùå Empty query. Please enter a valid prompt.\"\n",
        "    return query.lower(), None\n"
      ],
      "metadata": {
        "id": "rVuAHFQb1wqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MEDICAL_KEYWORDS = [\n",
        "    # Imaging Modalities\n",
        "    \"mri\", \"ct\", \"x-ray\", \"xray\", \"ultrasound\", \"radiograph\", \"radiography\", \"fluoroscopy\",\n",
        "    \"angiography\", \"pet\", \"spect\", \"echocardiogram\", \"ecg\", \"eeg\", \"emg\", \"endoscopy\",\n",
        "    \"colonoscopy\", \"laparoscopy\", \"biopsy\", \"mammogram\", \"microscopy\", \"histopathology\",\n",
        "    \"radiology\", \"tomography\", \"scan\", \"imaging\", \"radiation\",\n",
        "\n",
        "    # Organs / Body Parts\n",
        "    \"brain\", \"heart\", \"lung\", \"liver\", \"kidney\", \"pancreas\", \"spleen\", \"stomach\", \"intestine\",\n",
        "    \"colon\", \"esophagus\", \"bladder\", \"uterus\", \"ovary\", \"testis\", \"spine\", \"spinal\", \"eye\",\n",
        "    \"retina\", \"cornea\", \"ear\", \"nose\", \"throat\", \"thyroid\", \"bone\", \"joint\", \"muscle\", \"skin\",\n",
        "    \"nerve\", \"blood\", \"vessel\", \"artery\", \"vein\", \"gland\", \"cartilage\", \"ligament\", \"pelvis\",\n",
        "    \"hip\", \"knee\", \"shoulder\", \"hand\", \"wrist\", \"foot\", \"ankle\", \"neck\", \"chest\", \"abdomen\",\n",
        "\n",
        "    # Diseases / Conditions\n",
        "    \"tumor\", \"cancer\", \"carcinoma\", \"sarcoma\", \"melanoma\", \"leukemia\", \"lymphoma\", \"adenoma\",\n",
        "    \"metastasis\", \"infection\", \"pneumonia\", \"tuberculosis\", \"asthma\", \"covid\", \"influenza\",\n",
        "    \"diabetes\", \"hypertension\", \"stroke\", \"aneurysm\", \"infarction\", \"thrombosis\", \"embolism\",\n",
        "    \"arthritis\", \"osteoporosis\", \"fracture\", \"scoliosis\", \"meningitis\", \"encephalitis\",\n",
        "    \"hepatitis\", \"cirrhosis\", \"renal\", \"nephritis\", \"gastritis\", \"ulcer\", \"appendicitis\",\n",
        "    \"colitis\", \"bronchitis\", \"dermatitis\", \"eczema\", \"psoriasis\", \"glaucoma\", \"cataract\",\n",
        "    \"anemia\", \"obesity\", \"sepsis\", \"trauma\", \"injury\", \"lesion\", \"inflammation\", \"edema\",\n",
        "    \"necrosis\", \"fibrosis\", \"degeneration\", \"atrophy\", \"infection\", \"tumour\", \"swelling\",\n",
        "\n",
        "    # Procedures / Treatments\n",
        "    \"surgery\", \"operation\", \"transplant\", \"resection\", \"therapy\", \"chemotherapy\", \"radiation\",\n",
        "    \"dialysis\", \"immunotherapy\", \"gene therapy\", \"stem cell\", \"vaccination\", \"anesthesia\",\n",
        "    \"biopsy\", \"stent\", \"catheter\", \"implant\", \"prosthesis\", \"rehabilitation\", \"screening\",\n",
        "    \"monitoring\", \"examination\", \"diagnosis\", \"treatment\", \"therapy\", \"management\",\n",
        "\n",
        "    # General Medical / Research Terms\n",
        "    \"clinical\", \"pathology\", \"histology\", \"physiology\", \"anatomy\", \"genetic\", \"genome\",\n",
        "    \"mutation\", \"biomarker\", \"epidemiology\", \"toxicology\", \"oncology\", \"cardiology\",\n",
        "    \"neurology\", \"neuroscience\", \"gastroenterology\", \"urology\", \"orthopedic\", \"dermatology\",\n",
        "    \"ophthalmology\", \"otolaryngology\", \"pulmonology\", \"endocrinology\", \"immunology\",\n",
        "    \"hematology\", \"radiotherapy\", \"psychology\", \"psychiatry\", \"pharmacology\", \"vaccine\",\n",
        "    \"virus\", \"bacteria\", \"fungus\", \"parasite\", \"microbiology\", \"virology\", \"bioinformatics\",\n",
        "    \"biomedical\", \"medical\", \"clinical study\", \"symptom\", \"diagnostic\", \"scan report\",\n",
        "    \"laboratory\", \"tissue\", \"cell\", \"organism\", \"disease\", \"syndrome\", \"disorder\", \"case study\"\n",
        "]\n",
        "\n",
        "def is_medical_term(query):\n",
        "    \"\"\"Check if the user query likely relates to medical content.\"\"\"\n",
        "    return any(word in query.lower() for word in MEDICAL_KEYWORDS)"
      ],
      "metadata": {
        "id": "ghx-JESM11Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, 'r') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "annotations = load_jsonl(ANNOT_PATH)\n",
        "print(f\"‚úÖ Loaded {len(annotations)} dataset entries.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghvhjh5v15L1",
        "outputId": "9c8444d8-82f2-40a7-973f-25e74f0eb53d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 2118 dataset entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† Full Fine-Tuning of CLIP on MedICaT (High-Accuracy Version)\n",
        "!pip install open_clip_torch tqdm pillow --quiet\n",
        "\n",
        "import torch, json, os, random\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import open_clip\n",
        "\n",
        "# ---------------- Paths ----------------\n",
        "IMAGE_DIR = \"/content/drive/MyDrive/MediCat/figures\"\n",
        "ANNOT_PATH = \"/content/drive/MyDrive/MediCat/subcaptions_public.jsonl\"\n",
        "SAVE_DIR  = \"/content/drive/MyDrive/MediCat/finetuned_medclip_full\"\n",
        "\n",
        "# ---------------- Load dataset ----------------\n",
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "anns = load_jsonl(ANNOT_PATH)\n",
        "print(f\"Loaded {len(anns)} annotations\")\n",
        "\n",
        "pairs = []\n",
        "for ann in anns:\n",
        "    fig_uri, pdf_hash = ann.get(\"fig_uri\"), ann.get(\"pdf_hash\")\n",
        "    if not (fig_uri and pdf_hash):\n",
        "        continue\n",
        "    filename = f\"{pdf_hash}_{fig_uri}\"\n",
        "    img_path = os.path.join(IMAGE_DIR, filename)\n",
        "    if os.path.exists(img_path):\n",
        "        caption = ann.get(\"text\", \"\").strip()\n",
        "        if caption:\n",
        "            pairs.append((img_path, caption))\n",
        "\n",
        "print(f\"Prepared {len(pairs)} image‚Äìtext pairs for training\")\n",
        "\n",
        "# ---------------- Initialize CLIP ----------------\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
        "tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.train()  # fine-tune both image & text towers\n",
        "\n",
        "# ---------------- Training setup ----------------\n",
        "batch_size = 16\n",
        "epochs = 2\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "def get_batch():\n",
        "    \"\"\"Yield random mini-batches\"\"\"\n",
        "    batch = random.sample(pairs, batch_size)\n",
        "    imgs = [preprocess(Image.open(p[0]).convert(\"RGB\")) for p in batch]\n",
        "    caps = tokenizer([p[1] for p in batch])\n",
        "    return torch.stack(imgs).to(device), caps.to(device)\n",
        "\n",
        "steps_per_epoch = len(pairs) // batch_size\n",
        "print(f\"Training for {epochs} epochs √ó {steps_per_epoch} steps / epoch\")\n",
        "\n",
        "# ---------------- Training loop ----------------\n",
        "for epoch in range(epochs):\n",
        "    running = 0.0\n",
        "    for step in tqdm(range(steps_per_epoch), desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "        imgs, caps = get_batch()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            img_feat = model.encode_image(imgs)\n",
        "            txt_feat = model.encode_text(caps)\n",
        "            img_feat = img_feat / img_feat.norm(dim=1, keepdim=True)\n",
        "            txt_feat = txt_feat / txt_feat.norm(dim=1, keepdim=True)\n",
        "            logits_i = img_feat @ txt_feat.T\n",
        "            logits_t = logits_i.T\n",
        "            labels = torch.arange(len(imgs), device=device)\n",
        "            loss_i = loss_fn(logits_i, labels)\n",
        "            loss_t = loss_fn(logits_t, labels)\n",
        "            loss = (loss_i + loss_t) / 2\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} avg loss = {running/steps_per_epoch:.4f}\")\n",
        "\n",
        "# ---------------- Save model ----------------\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"medclip_finetuned_full.pt\"))\n",
        "print(f\"‚úÖ Saved fine-tuned CLIP model to: {SAVE_DIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451,
          "referenced_widgets": [
            "6a12545d86554b8dbdc2d40429543446",
            "ee21f3a734174b7d8d28ad80ea6396ef",
            "8530792dc4f5451fb7b8571eb8b82710",
            "7757d2ad52b344418dc0d1680da84a4d",
            "d9b3be75ac7b4223ae3c7ddcde5c325d",
            "2310c4196e5b4a7ea495f20b01c36a4d",
            "e3a4f0c4039044ebbd1074983d6c1147",
            "109138de098e44b2b0131d08fd758de0",
            "4e597fc19aa44a92af1056901c7bb281",
            "2009258dd6ee4dcb9a17496fa286aa3c",
            "e28ad1a9fffa4f3b8924dbe6efc3dc7b"
          ]
        },
        "id": "i5p8FoR82F9T",
        "outputId": "2b63035d-f57b-4768-8821-b4e8f050ed25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLoaded 2118 annotations\n",
            "Prepared 69 image‚Äìtext pairs for training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a12545d86554b8dbdc2d40429543446"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2192858961.py:52: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 2 epochs √ó 4 steps / epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/2:   0%|          | 0/4 [00:00<?, ?it/s]/tmp/ipython-input-2192858961.py:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch 1/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:44<00:00, 11.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 avg loss = 2.7146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:20<00:00,  5.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 avg loss = 2.6301\n",
            "‚úÖ Saved fine-tuned CLIP model to: /content/drive/MyDrive/MediCat/finetuned_medclip_full\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Enhanced CLIP-based indexing & retrieval (auto-detect fine-tuned model) ----------\n",
        "from sentence_transformers import util\n",
        "from PIL import Image\n",
        "import os, torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import open_clip\n",
        "\n",
        "# --- Auto-detect which fine-tuned model exists ---\n",
        "fast_model_path = \"/content/drive/MyDrive/MediCat/finetuned_medclip/medclip_finetuned.pt\"\n",
        "full_model_path = \"/content/drive/MyDrive/MediCat/finetuned_medclip_full/medclip_finetuned_full.pt\"\n",
        "\n",
        "if os.path.exists(full_model_path):\n",
        "    model_path = full_model_path\n",
        "elif os.path.exists(fast_model_path):\n",
        "    model_path = fast_model_path\n",
        "else:\n",
        "    raise FileNotFoundError(\"‚ùå No fine-tuned CLIP model found. Please run the fine-tuning cell first.\")\n",
        "\n",
        "print(\"‚úÖ Loading fine-tuned CLIP model from:\", model_path)\n",
        "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')  # ‚úÖ Important: keep pretrained='openai'\n",
        "model.load_state_dict(torch.load(model_path, map_location=\"cpu\"), strict=False)\n",
        "model.eval()\n",
        "clip_model = model\n",
        "\n",
        "print(\"‚úÖ CLIP model loaded and ready.\\n\")\n",
        "\n",
        "# --- Helper: build subcaption text ---\n",
        "def extract_subcaptions_from_ann(ann):\n",
        "    tokens = ann.get(\"tokens\", [])\n",
        "    token_texts = [t.get(\"text\", \"\") for t in tokens]\n",
        "    subcaptions = {}\n",
        "    sc_raw = ann.get(\"subcaptions\", {}) or {}\n",
        "    for label, idxs in sc_raw.items():\n",
        "        try:\n",
        "            words = [token_texts[i] for i in idxs if i < len(token_texts)]\n",
        "            text = \" \".join(words).replace(\" .\", \".\").replace(\" ,\", \",\").strip()\n",
        "            subcaptions[label] = text\n",
        "        except Exception:\n",
        "            subcaptions[label] = \"\"\n",
        "    return subcaptions\n",
        "\n",
        "# --- Helper: get image embedding ---\n",
        "def get_image_embedding_once(img_path):\n",
        "    try:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img_tensor = preprocess(img).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            emb = clip_model.encode_image(img_tensor)\n",
        "        return emb\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Image load error:\", img_path, e)\n",
        "        return None\n",
        "\n",
        "# --- Build improved subfigure index ---\n",
        "image_index = []\n",
        "fig_image_emb_cache = {}\n",
        "INDEX_LIMIT = 400   # can increase later\n",
        "\n",
        "print(\"‚öôÔ∏è Building CLIP subfigure index...\")\n",
        "for ann in tqdm(annotations[:INDEX_LIMIT], desc=\"Indexing Figures\"):\n",
        "    fig_uri = ann.get(\"fig_uri\")\n",
        "    pdf_hash = ann.get(\"pdf_hash\")\n",
        "    if not fig_uri or not pdf_hash:\n",
        "        continue\n",
        "    filename = f\"{pdf_hash}_{fig_uri}\"\n",
        "    img_path = os.path.join(IMAGE_DIR, filename)\n",
        "    if not os.path.exists(img_path):\n",
        "        alt_path = os.path.splitext(img_path)[0] + \".jpg\"\n",
        "        if os.path.exists(alt_path):\n",
        "            img_path = alt_path\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    # cache whole-figure embedding\n",
        "    if img_path in fig_image_emb_cache:\n",
        "        fig_img_emb = fig_image_emb_cache[img_path]\n",
        "    else:\n",
        "        fig_img_emb = get_image_embedding_once(img_path)\n",
        "        if fig_img_emb is None:\n",
        "            continue\n",
        "        fig_image_emb_cache[img_path] = fig_img_emb\n",
        "\n",
        "    fig_caption = ann.get(\"text\", \"\").strip()\n",
        "    subcaptions = extract_subcaptions_from_ann(ann)\n",
        "    subfigs = ann.get(\"subfigures\", []) or []\n",
        "\n",
        "    if subfigs:\n",
        "        for s in subfigs:\n",
        "            label = s.get(\"label\", \"\")\n",
        "            subcap_text = subcaptions.get(label, \"\").strip()\n",
        "            if not subcap_text:\n",
        "                subcap_text = (fig_caption.split(\".\")[0] + \".\").strip() if fig_caption else \"\"\n",
        "            combined_text = (subcap_text + \" \" + (fig_caption if fig_caption and fig_caption not in subcap_text else \"\")).strip()\n",
        "\n",
        "            txt_emb = clip_model.encode_text(open_clip.get_tokenizer('ViT-B-32')([combined_text]))\n",
        "            image_index.append({\n",
        "                \"id\": ann.get(\"fig_key\", \"\"),\n",
        "                \"fig_path\": img_path,\n",
        "                \"subfig_label\": label,\n",
        "                \"caption\": fig_caption,\n",
        "                \"inline_ref\": subcap_text,\n",
        "                \"combined_text\": combined_text,\n",
        "                \"img_emb\": fig_img_emb,\n",
        "                \"txt_emb\": txt_emb\n",
        "            })\n",
        "    else:\n",
        "        combined_text = fig_caption\n",
        "        if not combined_text:\n",
        "            continue\n",
        "        txt_emb = clip_model.encode_text(open_clip.get_tokenizer('ViT-B-32')([combined_text]))\n",
        "        image_index.append({\n",
        "            \"id\": ann.get(\"fig_key\", \"\"),\n",
        "            \"fig_path\": img_path,\n",
        "            \"subfig_label\": \"\",\n",
        "            \"caption\": fig_caption,\n",
        "            \"inline_ref\": fig_caption.split(\".\")[0] if fig_caption else \"\",\n",
        "            \"combined_text\": combined_text,\n",
        "            \"img_emb\": fig_img_emb,\n",
        "            \"txt_emb\": txt_emb\n",
        "        })\n",
        "\n",
        "print(f\"‚úÖ Indexed {len(image_index)} figure/subfigure entries (limit {INDEX_LIMIT}).\")\n",
        "\n",
        "# --- Retrieval: weighted text + image similarity ---\n",
        "def search_images(query, top_k=5, text_weight=0.75, image_weight=0.25):\n",
        "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "    q_tokens = tokenizer([query])\n",
        "    with torch.no_grad():\n",
        "        q_emb = clip_model.encode_text(q_tokens)\n",
        "    q_emb = q_emb / q_emb.norm(dim=1, keepdim=True)\n",
        "\n",
        "    scores = []\n",
        "    for entry in image_index:\n",
        "        t_emb = entry[\"txt_emb\"] / entry[\"txt_emb\"].norm(dim=1, keepdim=True)\n",
        "        i_emb = entry[\"img_emb\"] / entry[\"img_emb\"].norm(dim=1, keepdim=True)\n",
        "        t_sim = torch.matmul(q_emb, t_emb.T).item()\n",
        "        i_sim = torch.matmul(q_emb, i_emb.T).item()\n",
        "        combined = text_weight * t_sim + image_weight * i_sim\n",
        "        scores.append((combined, t_sim, i_sim, entry))\n",
        "\n",
        "    top = sorted(scores, key=lambda x: x[0], reverse=True)[:top_k]\n",
        "    results = []\n",
        "    for combined, t_sim, i_sim, e in top:\n",
        "        results.append({\n",
        "            \"path\": e[\"fig_path\"],\n",
        "            \"subfig_label\": e[\"subfig_label\"],\n",
        "            \"caption\": e[\"caption\"],\n",
        "            \"inline_ref\": e[\"inline_ref\"],\n",
        "            \"score_combined\": round(float(combined), 4),\n",
        "            \"score_text\": round(float(t_sim), 4),\n",
        "            \"score_image\": round(float(i_sim), 4)\n",
        "        })\n",
        "    return results\n",
        "\n",
        "print(\"üéØ Retrieval system ready (subfigure-based, fine-tuned CLIP).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow2CNfwz3JyG",
        "outputId": "ca9d0cdd-d25e-4ff7-d15e-a3b16ce72d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loading fine-tuned CLIP model from: /content/drive/MyDrive/MediCat/finetuned_medclip_full/medclip_finetuned_full.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ CLIP model loaded and ready.\n",
            "\n",
            "‚öôÔ∏è Building CLIP subfigure index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Indexing Figures: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:12<00:00, 30.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Indexed 37 figure/subfigure entries (limit 400).\n",
            "üéØ Retrieval system ready (subfigure-based, fine-tuned CLIP).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# üîß Create and cache a fallback image\n",
        "FALLBACK_PATH = \"/content/fallback_placeholder.png\"\n",
        "if not os.path.exists(FALLBACK_PATH):\n",
        "    img = Image.new(\"RGB\", (400, 300), color=(220, 220, 220))\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    draw.text((100, 140), \"No Image\", fill=(0, 0, 0))\n",
        "    img.save(FALLBACK_PATH)\n",
        "\n",
        "def medicat_search(query):\n",
        "    \"\"\"\n",
        "    Safely handles user query and returns valid image-caption pairs for Gradio.\n",
        "    Always returns actual images ‚Äî never None.\n",
        "    \"\"\"\n",
        "    query, error = handle_request(query)\n",
        "    if error:\n",
        "        return [(FALLBACK_PATH, error)]\n",
        "\n",
        "    if not is_medical_term(query):\n",
        "        return [(FALLBACK_PATH, \"‚ö†Ô∏è Not a medical term or context. Try another query.\")]\n",
        "\n",
        "    try:\n",
        "        query_emb = get_text_embedding(query)\n",
        "        results = retrieve_references(query_emb)\n",
        "    except Exception as e:\n",
        "        return [(FALLBACK_PATH, f\"‚ùå Retrieval error: {str(e)}\")]\n",
        "\n",
        "    output = []\n",
        "\n",
        "    for res in results:\n",
        "        # Handle tuple or dict\n",
        "        if isinstance(res, tuple) and len(res) == 2:\n",
        "            score, item = res\n",
        "        elif isinstance(res, dict):\n",
        "            score, item = res.get(\"score_combined\", 0.0), res\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # Safe path extraction\n",
        "        img_path = None\n",
        "        for key in (\"path\", \"fig_path\", \"image_path\"):\n",
        "            val = item.get(key)\n",
        "            if val and os.path.exists(val):\n",
        "                img_path = val\n",
        "                break\n",
        "\n",
        "        # Skip or fallback to placeholder\n",
        "        if not img_path:\n",
        "            img_path = FALLBACK_PATH\n",
        "\n",
        "        caption = item.get(\"caption\", \"No caption available\")\n",
        "        inline_ref = item.get(\"inline_ref\", \"\")\n",
        "        similarity = round(float(score) * 100, 2) if isinstance(score, (float, int)) else 0.0\n",
        "\n",
        "        caption_text = (\n",
        "            f\"ü©ª Caption: {caption}\\n\"\n",
        "            f\"üìñ Inline Ref: {inline_ref or 'None'}\\n\"\n",
        "            f\"üéØ Match Score: {similarity}%\"\n",
        "        )\n",
        "\n",
        "        output.append((img_path, caption_text))\n",
        "\n",
        "    # Always return at least one valid image\n",
        "    if not output:\n",
        "        output = [(FALLBACK_PATH, f\"‚ö†Ô∏è No valid image matches found for query '{query}'.\")]\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "13tHziWp36vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=medicat_search,\n",
        "    inputs=gr.Textbox(label=\"üîé Enter Medical Query\", placeholder=\"e.g., MRI images related to brain tumor\"),\n",
        "    outputs=gr.Gallery(label=\"Retrieved Medical Images with Captions\"),\n",
        "    title=\"üß† MedICaT Image‚ÄìText Retrieval System\",\n",
        "    description=\"Architecture Flow: User Layer ‚Üí Application Layer ‚Üí Data Layer ‚Üí Output Layer\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "DXsgLbFV4ASL",
        "outputId": "b6fc50ad-4b36-4504-83f5-bd1856936668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://0e454484130b896e72.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0e454484130b896e72.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://0e454484130b896e72.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}