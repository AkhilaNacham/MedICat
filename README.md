# MedICaT: A Dataset of Medical Images, Captions, and Textual References

The MedICaT paper introduces a large-scale dataset aimed at supporting multimodal learning in the biomedical domain by providing a rich collection of medical images and their corresponding textual descriptions. Sourced from over 131,000 open-access biomedical research papers, the dataset contains approximately 217,000 medical figures, including their original captions and the surrounding textual references that appear in the main body of the papers. One of the key contributions of the dataset is a manually annotated subset containing 7,500 subfigure-to-subcaption pairs, which addresses the challenge of compound figures that commonly appear in biomedical literature. The authors highlight the dataset√Ωs potential for advancing tasks like subfigure classification, figure-caption alignment, and retrieval. MedICaT fills an important gap in biomedical AI by enabling joint reasoning over visual and textual modalities, making it especially valuable for training and evaluating vision-language models in the medical domain.
